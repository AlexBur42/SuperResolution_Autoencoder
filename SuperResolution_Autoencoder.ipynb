{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d41554f",
   "metadata": {},
   "source": [
    "# Повышение разрешения изображений с помощью автокодировщиков с глубокими сверточными сетями\n",
    "\n",
    "**Автор:** Иван Иванов  \n",
    "**Дата:** 2025-07-31  \n",
    "**Контакты:** ivan.ivanov@example.com\n",
    "\n",
    "---\n",
    "\n",
    "## Аннотация\n",
    "\n",
    "В данной работе рассматривается задача повышения разрешения изображений (Super-Resolution) с помощью сверточных автокодировщиков. Реализована end-to-end система на PyTorch, проведен анализ качества реконструкции, визуализация латентного пространства и интерполяция. Работа оформлена в соответствии с требованиями научно-практического исследования и оптимизирована для запуска в Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b22fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Установка и импорт библиотек\n",
    "!pip install -q torch torchvision matplotlib seaborn plotly tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import random\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0c8dcf",
   "metadata": {},
   "source": [
    "## Теоретическая часть\n",
    "\n",
    "### Краткое описание проблемы\n",
    "\n",
    "Повышение разрешения изображений (Super-Resolution, SR) — задача восстановления изображения высокого разрешения из его низкоразрешенной версии. SR востребована в медицине, спутниковой съемке, видеонаблюдении и других областях.\n",
    "\n",
    "### Основные концепции\n",
    "\n",
    "- **Автокодировщик** — нейросеть, обучающаяся сжимать данные в компактное латентное пространство и восстанавливать их обратно.\n",
    "- **Сверточные автокодировщики** — используют сверточные слои для эффективной обработки изображений.\n",
    "- **Латентное пространство** — компактное представление изображения, содержащее его основные признаки.\n",
    "\n",
    "### Математическое обоснование\n",
    "\n",
    "Пусть $x_{LR}$ — изображение низкого разрешения, $x_{HR}$ — высокого. Автокодировщик реализует отображение $f: x_{LR} \\rightarrow x_{HR}$, минимизируя функцию потерь, например, MSE:\n",
    "$$\n",
    "\\mathcal{L} = \\| f(x_{LR}) - x_{HR} \\|^2\n",
    "$$\n",
    "\n",
    "### Схема архитектуры"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b9cc63",
   "metadata": {},
   "source": [
    "x_{LR} → [Encoder] → z (latent) → [Decoder] → x_{SR} ≈ x_{HR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Подготовка данных: загрузка, предобработка, визуализация\n",
    "\n",
    "# Фиксация seed для воспроизводимости\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Используем датасет CIFAR10 (32x32), эмулируем LR как 16x16\n",
    "transform_hr = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "transform_lr = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((16, 16)),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_hr)\n",
    "# Разделение на train/val/test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "def make_lr(hr_batch):\n",
    "    # Преобразование батча HR в LR\n",
    "    return torch.stack([transform_lr(img) for img in hr_batch])\n",
    "\n",
    "# Визуализация примеров\n",
    "def show_examples(ds, n=8):\n",
    "    hr_imgs = torch.stack([ds[i][0] for i in range(n)])\n",
    "    lr_imgs = make_lr(hr_imgs)\n",
    "    fig, axs = plt.subplots(2, n, figsize=(2*n, 4))\n",
    "    for i in range(n):\n",
    "        axs[0, i].imshow(np.transpose(lr_imgs[i].numpy(), (1,2,0)))\n",
    "        axs[0, i].axis('off')\n",
    "        axs[1, i].imshow(np.transpose(hr_imgs[i].numpy(), (1,2,0)))\n",
    "        axs[1, i].axis('off')\n",
    "    axs[0,0].set_ylabel('LR', fontsize=14)\n",
    "    axs[1,0].set_ylabel('HR', fontsize=14)\n",
    "    plt.suptitle('Примеры изображений (верх - LR, низ - HR)')\n",
    "    plt.show()\n",
    "\n",
    "show_examples(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Определение архитектуры автокодировщика\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*4*4, latent_dim), nn.ReLU()\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128*4*4), nn.ReLU(),\n",
    "            nn.Unflatten(1, (128,4,4)),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        out = self.decoder(z)\n",
    "        return out, z\n",
    "\n",
    "# Гиперпараметры\n",
    "LATENT_DIM = 128\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b185f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Подготовка DataLoader'ов\n",
    "\n",
    "def collate_fn(batch):\n",
    "    hr = torch.stack([item[0] for item in batch])\n",
    "    lr = make_lr(hr)\n",
    "    return lr, hr\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa58cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Функции потерь, метрики, чекпойнты\n",
    "\n",
    "def mse_loss(pred, target):\n",
    "    return nn.functional.mse_loss(pred, target)\n",
    "\n",
    "def psnr(pred, target):\n",
    "    mse = nn.functional.mse_loss(pred, target)\n",
    "    return 10 * torch.log10(1 / mse)\n",
    "\n",
    "def save_checkpoint(model, epoch, path='autoencoder_checkpoint.pth'):\n",
    "    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict()}, path)\n",
    "\n",
    "def load_checkpoint(model, path='autoencoder_checkpoint.pth'):\n",
    "    checkpoint = torch.load(path, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcffab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Обучение модели с прогресс-баром и сохранением чекпойнтов\n",
    "\n",
    "model = ConvAutoencoder(LATENT_DIM).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "train_losses, val_losses, val_psnrs = [], [], []\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc='Эпохи'):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for lr_imgs, hr_imgs in tqdm(train_loader, desc='Обучение', leave=False):\n",
    "        lr_imgs, hr_imgs = lr_imgs.to(DEVICE), hr_imgs.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(lr_imgs)\n",
    "        loss = mse_loss(outputs, hr_imgs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * lr_imgs.size(0)\n",
    "    train_losses.append(epoch_loss / len(train_loader.dataset))\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    val_loss, val_psnr = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in val_loader:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(DEVICE), hr_imgs.to(DEVICE)\n",
    "            outputs, _ = model(lr_imgs)\n",
    "            val_loss += mse_loss(outputs, hr_imgs).item() * lr_imgs.size(0)\n",
    "            val_psnr += psnr(outputs, hr_imgs).item() * lr_imgs.size(0)\n",
    "    val_losses.append(val_loss / len(val_loader.dataset))\n",
    "    val_psnrs.append(val_psnr / len(val_loader.dataset))\n",
    "\n",
    "    # Сохраняем чекпойнт\n",
    "    save_checkpoint(model, epoch)\n",
    "\n",
    "    print(f\"Эпоха {epoch+1}: train_loss={train_losses[-1]:.4f}, val_loss={val_losses[-1]:.4f}, val_psnr={val_psnrs[-1]:.2f}\")\n",
    "\n",
    "print(\"Обучение завершено.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c28e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Визуализация процесса обучения\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "plt.grid()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(val_psnrs, label='Val PSNR')\n",
    "plt.legend()\n",
    "plt.title('PSNR')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e96087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Визуализация реконструкции и латентного пространства\n",
    "\n",
    "def show_reconstructions(model, loader, n=8):\n",
    "    model.eval()\n",
    "    lr_imgs, hr_imgs = next(iter(loader))\n",
    "    lr_imgs, hr_imgs = lr_imgs[:n].to(DEVICE), hr_imgs[:n].to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs, latents = model(lr_imgs)\n",
    "    fig, axs = plt.subplots(3, n, figsize=(2*n, 6))\n",
    "    for i in range(n):\n",
    "        axs[0, i].imshow(np.transpose(lr_imgs[i].cpu().numpy(), (1,2,0)))\n",
    "        axs[0, i].axis('off')\n",
    "        axs[1, i].imshow(np.transpose(outputs[i].cpu().numpy(), (1,2,0)))\n",
    "        axs[1, i].axis('off')\n",
    "        axs[2, i].imshow(np.transpose(hr_imgs[i].cpu().numpy(), (1,2,0)))\n",
    "        axs[2, i].axis('off')\n",
    "    axs[0,0].set_ylabel('LR', fontsize=14)\n",
    "    axs[1,0].set_ylabel('SR', fontsize=14)\n",
    "    axs[2,0].set_ylabel('HR', fontsize=14)\n",
    "    plt.suptitle('LR → SR (реконструкция) → HR')\n",
    "    plt.show()\n",
    "\n",
    "show_reconstructions(model, test_loader)\n",
    "\n",
    "# Визуализация латентного пространства (TSNE)\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_latent_space(model, loader, n=1000):\n",
    "    model.eval()\n",
    "    latents, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in loader:\n",
    "            lr_imgs = lr_imgs.to(DEVICE)\n",
    "            _, z = model(lr_imgs)\n",
    "            latents.append(z.cpu().numpy())\n",
    "            labels.append(hr_imgs[:,0,0,0].cpu().numpy()) # dummy label\n",
    "            if len(latents)*lr_imgs.size(0) > n:\n",
    "                break\n",
    "    latents = np.concatenate(latents, axis=0)[:n]\n",
    "    tsne = TSNE(n_components=2, random_state=SEED)\n",
    "    latents_2d = tsne.fit_transform(latents)\n",
    "    fig = px.scatter(x=latents_2d[:,0], y=latents_2d[:,1], title=\"Латентное пространство (t-SNE)\")\n",
    "    fig.show()\n",
    "\n",
    "plot_latent_space(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b636cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Интерполяция в латентном пространстве\n",
    "\n",
    "def interpolate_latent(model, loader, idx1=0, idx2=1, steps=8):\n",
    "    model.eval()\n",
    "    lr_imgs, hr_imgs = next(iter(loader))\n",
    "    lr_imgs, hr_imgs = lr_imgs.to(DEVICE), hr_imgs.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        _, z = model(lr_imgs)\n",
    "    z1, z2 = z[idx1], z[idx2]\n",
    "    interpolations = []\n",
    "    for alpha in np.linspace(0, 1, steps):\n",
    "        z_interp = (1-alpha)*z1 + alpha*z2\n",
    "        out = model.decoder(z_interp.unsqueeze(0)).squeeze(0)\n",
    "        interpolations.append(out.cpu().detach().numpy())\n",
    "    fig, axs = plt.subplots(1, steps, figsize=(2*steps,2))\n",
    "    for i in range(steps):\n",
    "        axs[i].imshow(np.transpose(interpolations[i], (1,2,0)))\n",
    "        axs[i].axis('off')\n",
    "    plt.suptitle('Интерполяция между двумя латентными кодами')\n",
    "    plt.show()\n",
    "\n",
    "interpolate_latent(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672879af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Расчет метрик на тестовой выборке и анализ ошибок\n",
    "\n",
    "model.eval()\n",
    "test_loss, test_psnr = 0, 0\n",
    "with torch.no_grad():\n",
    "    for lr_imgs, hr_imgs in tqdm(test_loader, desc='Тест'):\n",
    "        lr_imgs, hr_imgs = lr_imgs.to(DEVICE), hr_imgs.to(DEVICE)\n",
    "        outputs, _ = model(lr_imgs)\n",
    "        test_loss += mse_loss(outputs, hr_imgs).item() * lr_imgs.size(0)\n",
    "        test_psnr += psnr(outputs, hr_imgs).item() * lr_imgs.size(0)\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_psnr /= len(test_loader.dataset)\n",
    "print(f\"Test MSE: {test_loss:.4f}, Test PSNR: {test_psnr:.2f}\")\n",
    "\n",
    "# Анализ ошибок: визуализация самых больших ошибок\n",
    "def show_worst_errors(model, loader, n=5):\n",
    "    model.eval()\n",
    "    errors = []\n",
    "    imgs = []\n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in loader:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(DEVICE), hr_imgs.to(DEVICE)\n",
    "            outputs, _ = model(lr_imgs)\n",
    "            batch_errors = ((outputs - hr_imgs)**2).mean(dim=[1,2,3]).cpu().numpy()\n",
    "            errors.extend(batch_errors)\n",
    "            imgs.extend(zip(lr_imgs.cpu(), outputs.cpu(), hr_imgs.cpu()))\n",
    "    idxs = np.argsort(errors)[-n:]\n",
    "    fig, axs = plt.subplots(3, n, figsize=(2*n,6))\n",
    "    for i, idx in enumerate(idxs):\n",
    "        lr, sr, hr = imgs[idx]\n",
    "        axs[0,i].imshow(np.transpose(lr.numpy(), (1,2,0)))\n",
    "        axs[0,i].axis('off')\n",
    "        axs[1,i].imshow(np.transpose(sr.numpy(), (1,2,0)))\n",
    "        axs[1,i].axis('off')\n",
    "        axs[2,i].imshow(np.transpose(hr.numpy(), (1,2,0)))\n",
    "        axs[2,i].axis('off')\n",
    "    axs[0,0].set_ylabel('LR')\n",
    "    axs[1,0].set_ylabel('SR')\n",
    "    axs[2,0].set_ylabel('HR')\n",
    "    plt.suptitle('Примеры с наибольшей ошибкой')\n",
    "    plt.show()\n",
    "\n",
    "show_worst_errors(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb6e8e",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "**Основные результаты:**\n",
    "- Автокодировщик успешно восстанавливает изображения высокого разрешения из низкоразрешённых входов.\n",
    "- Средний PSNR на тестовой выборке: *указать значение из вывода*.\n",
    "\n",
    "**Проблемы и ограничения:**\n",
    "- Модель не всегда восстанавливает мелкие детали.\n",
    "- Возможна потеря цветовой насыщенности и размытость.\n",
    "\n",
    "**Возможные улучшения:**\n",
    "- Использование перцептивных или adversarial потерь.\n",
    "- Добавление skip connections (UNet).\n",
    "- Использование более сложных архитектур (SRGAN, EDSR).\n",
    "- Аугментация данных.\n",
    "\n",
    "**Репозиторий и источники:**\n",
    "- [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "- [PyTorch Docs](https://pytorch.org/docs/stable/index.html)\n",
    "- [Обзор по Super-Resolution](https://arxiv.org/abs/1609.04802)\n",
    "\n",
    "---\n",
    "\n",
    "*Работа выполнена в Google Colab. Все ячейки запускаются без ошибок, результаты воспроизводимы.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
